# Uncomment to run with debugging
DEBUG_API=true

# System message for the agent
LLM_CHAT_MODEL_SYSTEM_MESSAGE="You are GPTStonks Chat, a financial chatbot developed by GPTStonks that democratizes the access to financial data.\n"

# ID of the embedding model to use
# Example provided with local model, use "default" for OpenAI model
AUTOLLAMAINDEX_EMBEDDING_MODEL_ID=local:BAAI/bge-base-en-v1.5

# LLM to use. Format provider:model_id, where model_id is dependent on the provider.
# Example provided with gpt-3.5-turbo-0125
LLM_MODEL_ID=openai:gpt-3.5-turbo-0125
OPENAI_API_KEY=sk-...

# Context window when using llama.cpp models and local models
LLM_LLAMACPP_CONTEXT_WINDOW=8000
AUTOLLAMAINDEX_LLM_CONTEXT_WINDOW=8000

# Randomness in the sampling of the posterior of the LLM
# 0 - greedy sampling, 1 - posterior without modification
LLM_TEMPERATURE=0

# Max tokens to sample from LLM
LLM_MAX_TOKENS=512

# Description of the OpenBB chat tool
OPENBBCHAT_TOOL_DESCRIPTION="useful to get historical pricing tabular data. Input should be a description of the data to retrieve and the dates in ISO format."
WORLD_KNOWLEDGE_TOOL_DESCRIPTION="useful to solve complex or incomplete financial questions and to search on the Internet current events, news and concrete financial datapoints. Input must be an extended description of the information to search."
AUTOMULTISTEPQUERYENGINE_INDEX_SUMMARY="useful to search information on the Internet. Input must be an extended description of the information to search."
AUTOMULTISTEPQUERYENGINE_STEPDECOMPOSE_QUERY_PROMPT="The original question is as follows: '{query_str}'\nWe must answer this question from a knowledge source, by dividing it into simpler questions. Context information for the knowledge source is provided below, as well as previous reasoning steps.\nGiven the context and previous reasoning, return a relevant question that can be answered from the context and helps answer the original question. This relevant question can be the same as the original question if it is simple, or this question can represent a step towards answering the overall question. It must be relevant to answer the original question.\nIf we cannot extract more information from the context, provide 'None' as the answer, but NEVER if the previous reasoning is 'None' too.\n\nQuestion: {query_str}\nKnowledge source context: {context_str}\nPrevious reasoning: {prev_reasoning}\nNew question (can't be 'None'): "
AUTOMULTISTEPQUERYENGINE_QA_TEMPLATE="Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query. Write very detailed answers.\nQuery: {query_str}\nAnswer: "
AUTOMULTISTEPQUERYENGINE_REFINE_TEMPLATE="You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWrite very detailed answers.\nWhen in doubt, just repeat the original answer.\nNew Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: "

# Path to the Vector Store Index (VSI)
AUTOLLAMAINDEX_VSI_PATH=vsi:./gptstonks/api/data/openbb_v4.1.0_historical_vectorstoreindex_bgebaseen # name of Pinecone index (by default Pinecone is used), or local embeddings
# AUTOLLAMAINDEX_REMOTE_VECTOR_STORE_API_KEY=... # API key for Pinecone. Uncomment to use Pinecone, otherwise local embeddings

# Template for the QA (Question-Answer) format
AUTOLLAMAINDEX_QA_TEMPLATE="You must write Python code to solve the query '{query_str}'. You must use only one of the functions below and store its output in a variable called `res`.\n---------------------\n{context_str}\n---------------------\nWrite the Python code between '```python' and '```', using only one of the functions above. Do not use `print`."

# MongoDB URI
MONGO_URI=mongodb://mongo:27017

# MongoDB database name
MONGO_DBNAME=mongodb
